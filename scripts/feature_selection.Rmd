---
title: "Regression: Feature Selection"
author: "Will Doyle"
---

## Introduction




```{r}
library(tidyverse)
library(tidymodels)
```



## The Data

We'll use the Ames housing data, a dataset of house prices and characteristics for Ames Iowa. We'll try to predict house prices. 

```{r}
data(ames)
```


## Feature Engineering

Feature engineering is the Machine Learning term for what we would call data wrangling. Some of this can be done programatically, other parts more "by hand." The code below will select lot area, year built, rooms above ground, and ever variable that ends with "SF" which stands for square feet. It also logs sale price-- the dependent variable. 
```{r}
ames<-ames%>%
  select(Sale_Price,
    Lot_Area,
         Year_Built,
         TotRms_AbvGrd,
         ends_with("_SF"))%>%
  mutate(log_sale_price=log(Sale_Price))%>%
  select(-Sale_Price)
```

## Training and Testing

We'll split the data into training and testing, using the default 75/25 split. 

```{r}
split_data<-initial_split(ames)

ames_train<-training(split_data)

ames_test<-testing(split_data)
```

## OLS 

For the baseline example let's run OLS on the data

### Set workflow

`Tidymodels` relies on the idea of a modeling workflow. Let's start by defining an empty one. 

```{r}
ames_wf<-workflow()
```


### Model statement
Next we'll define a simple linear model via `lm`. 

```{r}
lm_fit<-
  linear_reg()%>%
  set_engine("lm")%>%
  set_mode("regression")
```


Then we can add the model to the workflow. 
```{r}
ames_wf<-ames_wf%>%
  add_model(lm_fit)
```

### Recipe

A recipe in tidymodels is the set of steps that need to be done to the data prior to analysis. The wonderful think about the recipe approach is that it works exactly the same on training and testing or resampled data. 

First we'll set a formula-- the one below says to create a model with log sale price on the LHS and everything else in the dataset on the RHS. 

```{r}
ames_formula<-as.formula("log_sale_price~.")
```

With that formula, we can then define the recipe. The recipe are the remaining feature engineering steps that need to occur. In this case we're going to define the dependent variable via `update_role`, then normalize the data via `step_normalize` then drop all missing data via `step_naomit.`

```{r}
ames_rec<-recipe(ames_formula,ames)%>%
  update_role(log_sale_price,new_role="outcome")%>%  ## specify dv
  step_normalize(all_predictors())%>% ## Convert all to Z scores
  step_naomit(all_predictors()) ## drop missing
```


Now we can add the recipe to the workflow.

```{r}
ames_wf<-ames_wf%>%
  add_recipe(ames_rec)
```

## Fit the model to the training data

Quite easily done via `fit`

```{r}
ames_wf<-ames_wf%>%
  fit(ames_train)
```

## Predict results in the testing data

Now we can use the estimates in the fitted model  to predict outcomes in the testing dataset. These will be called `.pred` until we rename them as `.pred1`, and add them to the dataset.  

```{r}
  ames_test<-
  ames_wf%>%
  predict(new_data=ames_test)%>%
  rename(.pred1=.pred)%>%
  bind_cols(ames_test)
```

## Calculate RMSE

Next we can use the `rmse` command to compare the actual log sale price in the dataset to the predicted log price. 

```{r}
ames_test%>%
  rmse(truth=log_sale_price,estimate=.pred1)
```


## Lasso model


## Lasso for Feature Selection

One of the key decisions for an analyst is which variables to include. We can make decisions about this using theory, or our understanding of the context, but we can also rely on computational approaches. This is known as *regularization* and it involves downweighting the importance of coefficients from a model based on the contribution that a predictor makes. We're going to make use of a regularization penalty known as the "lasso." The lasso downweights variables mostly be dropping variables that are highly correlated with one another, leaving only one of the correlated variables as contributors to the model. We set the degree to which this penalty will be implemented by setting the "penalty" variable in the model specification. 


Now we can update the model to use lasso, which will subset on a smaller number of covariates. In the `tidymodels` setup, ridge is alpha (mixture)=0, while lasso is alpha (mixture)=1.   https://parsnip.tidymodels.org/reference/glmnet-details.htm
```{r}
penalty_spec<-.1

mixture_spec<-1

lasso_fit<- 
  linear_reg(penalty=penalty_spec,
             mixture=mixture_spec) %>% 
  set_engine("glmnet")%>%
  set_mode("regression")
```

## Define the Workflow

```{r}
ames_wf<-workflow()
```

## Add the Model

```{r}
ames_wf<-ames_wf%>%
  add_model(lasso_fit)
```

## Set Formula

In setting the recipe for this model, we're now going to include ever variable in the dataset. This is very common in these kinds of applications. 

```{r}
ames_formula<-as.formula("log_sale_price~.")
```

## Recipe

Because we have so many predictors, we need to generalize our process for feature engineering. Instead of running steps on particular variables, we're going to use the capabilities of tidymodels to select types of variables. 


```{r}
ames_rec<-recipe(ames_formula,ames)%>%
  update_role(log_sale_price,new_role="outcome")%>%  ## specify dv
  step_normalize(all_predictors())%>% ## Convert all to Z scores
  step_naomit(all_predictors()) ## drop missing
```

To look at what a prepped dataset would look like, we can use the `prep`--> then `bake` commands. 

```{r}
ames_processed<-ames_rec%>%prep()%>%bake(ames_train)
```

Now we can add our recipe to the workflow. 

```{r}
ames_wf<-ames_wf%>%
  add_recipe(ames_rec)
```

Aad fit the data


```{r}
ames_wf<-ames_wf%>%
  fit(ames_train)
```

We can use the same set of commands as above to generate as prediction, which will be pred2 this time.

```{r}
  ames_test<-
  ames_wf%>%
  predict(new_data=ames_test)%>%
  rename(.pred2=.pred)%>%
  bind_cols(ames_test)
```

We can then check the rmse from our model. 
```{r}
ames_test%>%
  rmse(truth=log_sale_price,estimate=.pred2)
```

We can also look at the coefficients to get a sense of what got included and what got dropped. 
```{r}
ames_wf%>%
  extract_fit_parsnip()%>%
  tidy()
```



## Cross Validation

The essence of prediction is discovering the extent to which our models can predict outcomes for data that does not come from our sample. Many times this process is temporal. We fit a model to data from one time period, then take predictors from a subsequent time period to come up with a prediction in the future. For instance, we might use data on team performance to predict the likely winners and losers for upcoming soccer games. 

This process does not have to be temporal. We can also have data that is out of sample because it hadn't yet been collected when our first data was collected, or we can also have data that is out of sample because we designated it as out of sample.

The data that is used to generate our predictions is known as 
*training* data. The idea is that this is the data used to train our model, to let it know what the relationship is between our predictors and our outcome. So far, we have worked mostly with training data. 

That data that is used to validate our predictions is known as *testing* data. With testing data, we take our trained model and see how good it is at predicting outcomes using out of sample data. 

One very simple approach to this would be to cut our data in half. This is what we've done so far.  We could then train our model on half the data, then test it on the other half. This would tell us whether our measure of model fit (e.g. rmse, auc) is similar or different when we apply our model to out of sample data. 

But this would only be a "one-shot" approach. It would be better to do this multiple times, cutting the data into two parts: training and testing, then fitting the model to the training data, and then checking its predictions against the testing data. That way, we could generate a large number of rmse's to see how well the model fits on lots of different possible out-of-sample predictions. 

This process is called *cross validation*, and it involves two important decisions: first, how will the data be cut, and how many times will the validation run. 

We're going to cut our training dataset 75/25, and we'll repeat that 25 times. This is so our code will run faster-- we would really want to do this more like 1,000 times in practice. 

## Monte Carlo Resampling

The code below will generate a resampled dataset using monte carlo resampling. 

```{r}
ames_rs<-mc_cv(ames_train,times=25) ## More like 1000 in practice
```


We can then fit the model to the resampled data via `fit_resamples`.
```{r}
ames_lasso_fit<-ames_wf%>%
  fit_resamples(ames_rs)
```

The model has now been fit to 25 versions of the training data. Let's look at the metrics using `collect_metrics`.
```{r}
ames_lasso_fit%>%collect_metrics()
```


We can also pull certain metrics like rmse one at a time if we want.
```{r}
ames_lasso_fit%>%
  unnest(.metrics)%>%
  filter(.metric=="rmse")%>%
  ggplot(aes(x=.estimate))+
  geom_density()

```


## Model Tuning

The problem with the above is that I arbitrarily set the value of penalty to be .1. Do I know this was correct? No!
What we need to do is try out a bunch of different values of the penalty, and see which one gives us the best model fit. This
process has the impressive name of "hyperparameter tuning" but it could just as easily be called "trying a bunch of stuff to see what works."

Below I'm going to give the argument `tune()` for the value of penalty. This will allow us to "fill in" values later. 


Of course we don't know what penalty to use in the lasso model, so we can tune it. This is set up
by using the `penalty=tune()` approach below. 

```{r}
ames_tune_model<- 
  linear_reg(penalty=tune(),mixture=mixture_spec)%>% 
  set_engine("glmnet")
```

Now that we've said which parameter to tune, we'll use the `grid_regular` command to get a set of nicely spaced out values. This command is specific to the parameter, so it will choose reasonable values for a penalty. 

```{r}
lasso_grid<-grid_regular(parameters(ames_tune_model) ,levels=10)
```

We can use `update_model` to change our workflow with the new model. 
```{r}
ames_wf<-ames_wf%>%
  update_model(ames_tune_model)
```

Then we can use `tune_grid` to run the model through the resampled data, using the grid supplied. 
```{r}
ames_lasso_tune_fit <- 
  ames_wf %>%
    tune_grid(ames_rs,grid=lasso_grid)
```

## Examine Results

Lets' take a look and see which models fit better. 

```{r}
ames_lasso_tune_fit%>%
  collect_metrics()%>%
  filter(.metric=="rmse")%>%
  arrange(mean)
```

## Elastic Net

The elastic net model combines the lasso and the ridge using the mixture parameter. Let's just go ahead and tune on that as well.

```{r}
ames_wf<-workflow()
```

```{r}
ames_tune_fit<- 
  linear_reg(penalty=tune(),mixture=tune())%>% 
  set_engine("glmnet")
```

```{r}
enet_grid<-grid_regular(parameters(ames_tune_fit) ,levels=10)
```

We can use `add_model` to change our workflow with the new model. 
```{r}
ames_wf<-ames_wf%>%
  add_model(ames_tune_fit)
```

```{r}
ames_wf<-ames_wf%>%
  add_recipe(ames_rec)
```


Then we can use `tune_grid` to run the model through the resampled data, using the grid supplied. 
```{r}
ames_lasso_tune_fit <- 
  ames_wf %>%
    tune_grid(ames_rs,grid=enet_grid)
```

## Examine Results

Lets' take a look and see which models fit better. 

```{r}
ames_lasso_tune_fit%>%
  collect_metrics()%>%
  filter(.metric=="rmse")%>%
  arrange(mean)%>%
  print(n=100)


ames_lasso_tune_fit%>%show_best()
ames_lasso_tune_fit%>%select_best()
```


