---
title: "Neural Nets"
output: html_document
date: "2023-02-15"
---

## Fitting Neural Nets

A Neural net uses multiple layers to predict an outcome. The basic concept is that the inputs feed into a series of linear combinations (neurons), which are generated by an activation function. As with every other ML method, there are a series of hyperparameters that need to be tuned. 

## Libraries

```{r}

library(tidyverse)
library(tidymodels)
library(janitor)
library(tensorflow)
library(imager)

```


## Data 

```{r}
zip_train<-read_table("../data/zip.train",col_names = FALSE)%>%clean_names()%>%select(-x258)%>%
  mutate(label=as_factor(x1))%>%
  select(label, contains("x"),-x1)%>%
  mutate(split="analysis")

zip_test<-read_table("../data/zip.test",col_names = FALSE)%>%clean_names()%>%
  mutate(label=as_factor(x1))%>%
  select(label, contains("x"),-x1)%>%
  mutate(split="assessment")

zip_complete<-bind_rows(zip_train,zip_test)

split_data<-zip_complete%>%group_initial_split(split)

zip_train<-training(split_data)

zip_test<-testing(split_data)

```


```{r}
display_image <- function(data){
  message("Displaying: ", data$y)
  
  data %>% 
    select(-label) %>% 
    unlist(use.names = FALSE) %>% 
    as.cimg(x = 16, y = 16) %>% 
    plot(axes = FALSE)
}
```

```{r}
zip_train %>% 
  slice(50) %>% 
  display_image()
```



```{r}
zip_formula<-as.formula("label~.")
```

```{r}
zip_recipe<-recipe(zip_formula,zip_train)%>%
  step_normalize(all_predictors())
```

```{r}
mlogit_mod<-multinom_reg(mode="classification",
                         engine="glmnet",penalty = 0
                          )
```



```{r}
mlogit_wf<-workflow()%>%
  add_model(mlogit_mod)%>%
  add_recipe(zip_recipe)
```

```{r}
mlogit_result<-mlogit_wf%>%fit(zip_train)
```

```{r}
mlogit_result%>%
  predict(new_data = zip_test,type = "prob")%>%
  bind_cols(zip_test)%>%
  mn_log_loss(truth=label,contains("pred"))
```

```{r}
nnet_model<- mlp(hidden_units = tune(), 
                 epochs=tune(),
                 penalty=tune(),
                 learn_rate = tune(),
                 activation="relu") %>%
  set_mode("classification") %>% 
  set_engine("brulee")
```

```{r}
mlp_grid<-grid_regular(parameters(nnet_model),levels = c(3,3,3,3))
```


```{r}
nnet_wf<-workflow()%>%
  add_model(nnet_model)%>%
  add_recipe(zip_recipe)
```


```{r}
zip_cv<-mc_cv(zip_train)
```


```{r}
nnet_fit<-nnet_wf%>%
  tune_grid(resamples = zip_cv,
            grid=mlp_grid,
                metrics=metric_set(
                accuracy,
                mn_log_loss
                ),
                control=control_resamples(save_pred=TRUE))
```
```{r}
save(nnet_fit,file="nnet_fit.Rdata")
```



```{r}
# select best values for the tuning parameter
best_hyperparameters <- select_best(nnet_fit, metric = "mn_log_loss")

# finalize the workflow with those parameter values
final_wflow <- nnet_wf %>%
  finalize_workflow(best_hyperparameters)

# `last_fit()` for fit on training, predict on test, and report performance
lf <- last_fit(final_wflow, split_data)
lf%>%collect_metrics()

```





